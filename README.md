# writeup:
Harrison Gregg (Kevin Leutzinger)
Artificial Intelligence 
December 18, 2013
Final Project 

For the final project I worked with Kevin to create a program that used a neural net to attempt to recognize written digits.  As with any application of a neural net, the only challenge was getting input that we could feed into the neural net, although we did end up modifying my original neural net a little.  Originally, I wanted to do a final project that attempted to detect faces or some other object.  However, I realized that the time scale required to train the neural net on images with hundreds of pixels would be way to large.  Kevin had the idea of choosing something that could be contained within a much smaller image, and as we had discussed handwriting recognition in the class, we decided to try to implement this.  

To generate the input, we created a simple python script that would allow us to edit a 15x20 array of pixels and then save it as an array.  We started by generating a sample of 8s and not 8s to keep things simple at the beginning.  We didn't do anything fancy with the input and simply fed the pixels into the neural net as a one dimensional array.  In the future, we might try to figure out a way to give the data to the neural net such that it knows more about the proximity of different pixels.  The desired output for out training set is either a 1 or a 0, representing 8 or not 8 respectively.  The output of the neural net is therefore a number between 0 and 1 that represents how much the neural net thinks the numeber looks like an 8.  

Once we got this working, we simply repeated this process for the rest of the numbers 0-9.  So we had 10 different neural nets, each training on the same training set, only with different inputs marked as 1s and 0s.  This worked pretty well, however, it was obviously somewhat inefficient to have 10 different neural nets that all had to be trained individually.  To improve upon this inefficiency, we then generalized the neural net class for any number of output perceptrons.  This meant that we could have a smaller total number of hidden perceprons, as some of them were probably doing redundant tasks when we had multiple neural nets.  The training was also much more efficient, as the neural net was now always training one of the outputs positively instead of spending 90% of the time training each of the nets negatively.  

The performance of the neural net at this point is quite impressive in my opinion, though it is certainly not always accurate.  To improve its performance at this point, I think the next step should be increasing the sample size of our training set, which is currently quite limited.  However, as this will linearly increase the training time, we should probably figure out how to train the net faster.  There are two options that I would like to explore to do this.  The first is training the neural net in a more efficient language, such as C++.  Aaron has a working neural net written in C++, so we could probably borrow his code for this.  The second would be trying to get python to use my computers resources more efficiently.  My desktop computer has a quad core processor and a very nice graphics card, so it seems like a waste to be using only the default one core of the CPU.  I would like to experiment with both of these options.  It is very exciting to be doing programming that is actually using up enough resources that we need to worry about efficiency.

Whenever I work with neural nets, I am amazed their ability to do things without having to be explicitly programmed to do them.  I look forward to using them in my future for fun and hopefully some time to help me accomplish an otherwise difficult task.
